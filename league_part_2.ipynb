{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# League of Legends Competitive Match Data\n",
    "* **See the main project notebook for instructions to be sure you satisfy the rubric!**\n",
    "* See Project 03 for information on the dataset.\n",
    "* A few example prediction questions to pursue are listed below. However, don't limit yourself to them!\n",
    "    * Predict if a team will win or lose a game.\n",
    "    * Predict which role (top-lane, jungle, support, etc.) a player played given their post-game data.\n",
    "    * Predict how long a game will take before it happens.\n",
    "    * Predict which team will get the first Baron.\n",
    "\n",
    "Be careful to justify what information you would know at the \"time of prediction\" and train your model using only those features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Findings\n",
    "\n",
    "\n",
    "### Introduction\n",
    "Last time, I analyzed the League of Legends esports matching data using hypothesis test, to see if the Cloud Dragon Soul is as powerful as other Dragon Souls. This time, I will coninue the analysis of these esports data, but try some different topics of this game. One of the most common suspense people have when watching the esports games, or even all sports games, is that who is going to be the winner? In this article, I want to try to **predict the winner given the statistics of the matches at the in-game time 15th minute** (it is extremely rare that a match takes under 15 minutes, and I will cover this in the following sections). Since the winner of a match is a categorical data, this predition is a **classification** task.\n",
    "\n",
    "As a reminder, each row of the dataset I used stores the statistics of a single player or a team of only one match, and I will predict the winner of the game with only the team statistics. The target variable I am going to predict it the `'result'` column, which represents whether the team/player of this row wins this match. There's only two possible values, `1` means that the team wins, while `0` means the team loses.\n",
    "\n",
    "I would like to use **accuracy** as the evaluation metric of my prediction. Since the dataset has the statistics of both teams, there must be exactly one winner and one loser of each match. Therefore the results of matches should be **perfectly balanced**, exactly `50%/50%`, and accuracy should be a good metric here. I didn't choose recall or precesion because neither false negative or false positive is more important than the other here. The model is good as long as it has a good accuracy.\n",
    "\n",
    "### Baseline Model\n",
    "Again, before start doing the baseline model, I need to do some data cleaning here. As mentioned above, I would only use the team statistics, so firstly I removed all the rows where the column `'position'` is not `'team'`. There are `24418` rows remaining, and each match is stored as two rows for each team, meaning there are `12209` matches at total. Then I found that there are one match (two rows) that has `'gamelength'` less than 15 minutes (107 seconds to be exact). It is not reasonable that a game takes such a short time. There must be some problem that stopped the match, so I decided to remove these two rows. Also, I found that there are three matches that none of the two teams has the `'result'` column to be `1`, meaning there's no winner. This is also abnormal so I removed them. And finally, I removed all the irrelevant columns, and only kept the basic information of the match (`'result','patch','league','side'`) and the statistics at 15th minutes (columns ending with `'at15'`).\n",
    "\n",
    "I also noticed that there's some data missing. The statistics at 15 minutes is entirely missing as long as the rows have the value of column `'league'` to be `'LPL'` or `'LDL'` (thus they are **Missing by Design**). And very few statistics are also missing for the rows with `'league'` to be `'WCS'`. The imputation methods I've learned so far do not make too much sense in this situation, since they are not representing the real data and could possibly lower the evaluation metrics of my predictions. I decided to ignore them, and only train and test the model with the rest of the dataset.\n",
    "\n",
    "So my baseline model officially starts here. I used the Decision Tree Classifier (`sklearn.tree.DecisionTreeClassifier`) for the baseline model, and there are 7 features used: `'league', 'killsat15', 'assistsat15', 'deathsat15', 'opp_killsat15', 'opp_assistsat15', 'opp_deathsat15'`. The feature `'league'` represents the name of the esports association in a region. For example, the league in NA is called `'LCS'`. Different league has different gaming style. Some league might be doing better on reversing the early game disadvantage, thus influence the results. Since this feature has the data type string, and there's no inherent ordering between leagues, it should be a **nomial** data. I did an **one-hot-encoding** on this feature.\n",
    "\n",
    "The rest 6 features are all related to the kills and deaths on the team and its opponent team, which are the most intuitive statistics to see which team is more advantageous at the 15th minutes. If one team has more kills and less deaths than its opponents, then we can say this team is somewhat playing better. These features are all floats (**numeric**), so I **leave them as is**. In addition, I did not use any hyperparameter, all the parameters of the `DecisionTreeClassifier` are the default values.\n",
    "\n",
    "I splitted the dataset using `sklearn.model_selection.train_test_split` to make a 75% train set and a 25% test set. The accuracy I got from the baseline model is about `0.603`, with some random error every time the `train_test_split` runs.\n",
    "As mentioned the result column of this dataset is perfectly balanced, if I guess all the results to be 0 or 1, I would get a `0.5` accuracy. So the accuracy of this basline model, around `0.6`, is not so good in my opinion.\n",
    "\n",
    "### Final Model\n",
    "For the baseline model I only used the most intuitive statistics (kills, assists, and deaths) that represents the advantage of a team at 15th minutes. For the final model, I added some other statistics, `'golddiffat15','xpdiffat15','csdiffat15'`, which are more slight advantages we can observe in the game. These three statistics are the ones that every good League of Legends players pursuing for in every game. I also added the features `'patch'` (game version) and `'side'` (the team is at upper-right side or bottom-left side of the game map), since the game pace changes in every game patch, and in some patches, one side is more advantageous than the other side. \n",
    "\n",
    "For the final model, I am still using the DecisionTreeClassifier. But this time, I used the `sklearn.model_selection.GridSearchCV` to search for good hyperparameters of the Decision Tree. I listed several values for each of the parameters of Decision Tree `max_depth, min_samples_split`, and `criterion`. The parameters that performed best are: `criterion: 'gini', max_depth: 4`, and `min_samples_split: 2`. The accuracy ended up with `0.746` for both the train set and test set.\n",
    "\n",
    "### Fairness Evaluation\n",
    "The league `LCK`, located in South Korea, winned the Final Worlds Championship in 2022. When the gaming level of a team is better, it has a better chance to reverse the disadvantage in the early game. And I noticed that my final model has an accuracy of around `0.72` when predicting the matches made in `LCK`. Is my model more unfair when doing predictions on `LCK` than those on other leagues? I will do a permutation test on this.\n",
    "\n",
    "First I have the null and alternative hypothesis:\n",
    "- Null Hypothesis: my model is fair; the accuracy for my two subsets are roughly the same\n",
    "\n",
    "- Alternative Hypothesis: my model is unfair; the accuracy for the LCK subset is lower than the other leagues subset\n",
    "\n",
    "I picked the parity measure to be accuracy parity again, because we want to show the predictions are independent of `'league'`. I shuffled the column `'league'` for `500` times. For each shuffle, I splitted the dataset based on the column `'league'` to be `LCK` or others, made predictions with my final model, substract the other subset accuracy from `LCK` subset accuracy, and record it in a numpy array. After comparing the simulated accuracy difference and the observed difference, **P-value** ended up to be `0.258`. Despite it is small, but compared to the **standard 0.05 P-value**, it is not small enough for us to reject the null hypothesis. So the conclusion is that we **failed to reject the null hypothesis**, this observation might be happening by random chance. We cannot confidently say that my model is unfair."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'  # Higher resolution figures\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/haoyufu/opt/anaconda3/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3444: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gameid</th>\n",
       "      <th>datacompleteness</th>\n",
       "      <th>url</th>\n",
       "      <th>league</th>\n",
       "      <th>year</th>\n",
       "      <th>split</th>\n",
       "      <th>playoffs</th>\n",
       "      <th>date</th>\n",
       "      <th>game</th>\n",
       "      <th>patch</th>\n",
       "      <th>...</th>\n",
       "      <th>opp_csat15</th>\n",
       "      <th>golddiffat15</th>\n",
       "      <th>xpdiffat15</th>\n",
       "      <th>csdiffat15</th>\n",
       "      <th>killsat15</th>\n",
       "      <th>assistsat15</th>\n",
       "      <th>deathsat15</th>\n",
       "      <th>opp_killsat15</th>\n",
       "      <th>opp_assistsat15</th>\n",
       "      <th>opp_deathsat15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ESPORTSTMNT01_2690210</td>\n",
       "      <td>complete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LCK CL</td>\n",
       "      <td>2022</td>\n",
       "      <td>Spring</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-10 07:44:08</td>\n",
       "      <td>1</td>\n",
       "      <td>12.01</td>\n",
       "      <td>...</td>\n",
       "      <td>510.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>-1617.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ESPORTSTMNT01_2690210</td>\n",
       "      <td>complete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LCK CL</td>\n",
       "      <td>2022</td>\n",
       "      <td>Spring</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-10 07:44:08</td>\n",
       "      <td>1</td>\n",
       "      <td>12.01</td>\n",
       "      <td>...</td>\n",
       "      <td>487.0</td>\n",
       "      <td>-107.0</td>\n",
       "      <td>1617.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ESPORTSTMNT01_2690219</td>\n",
       "      <td>complete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LCK CL</td>\n",
       "      <td>2022</td>\n",
       "      <td>Spring</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-10 08:38:24</td>\n",
       "      <td>1</td>\n",
       "      <td>12.01</td>\n",
       "      <td>...</td>\n",
       "      <td>555.0</td>\n",
       "      <td>-1763.0</td>\n",
       "      <td>-906.0</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ESPORTSTMNT01_2690219</td>\n",
       "      <td>complete</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LCK CL</td>\n",
       "      <td>2022</td>\n",
       "      <td>Spring</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-10 08:38:24</td>\n",
       "      <td>1</td>\n",
       "      <td>12.01</td>\n",
       "      <td>...</td>\n",
       "      <td>533.0</td>\n",
       "      <td>1763.0</td>\n",
       "      <td>906.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>8401-8401_game_1</td>\n",
       "      <td>partial</td>\n",
       "      <td>https://lpl.qq.com/es/stats.shtml?bmid=8401</td>\n",
       "      <td>LPL</td>\n",
       "      <td>2022</td>\n",
       "      <td>Spring</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-01-10 09:24:26</td>\n",
       "      <td>1</td>\n",
       "      <td>12.01</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 123 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   gameid datacompleteness  \\\n",
       "10  ESPORTSTMNT01_2690210         complete   \n",
       "11  ESPORTSTMNT01_2690210         complete   \n",
       "22  ESPORTSTMNT01_2690219         complete   \n",
       "23  ESPORTSTMNT01_2690219         complete   \n",
       "34       8401-8401_game_1          partial   \n",
       "\n",
       "                                            url  league  year   split  \\\n",
       "10                                          NaN  LCK CL  2022  Spring   \n",
       "11                                          NaN  LCK CL  2022  Spring   \n",
       "22                                          NaN  LCK CL  2022  Spring   \n",
       "23                                          NaN  LCK CL  2022  Spring   \n",
       "34  https://lpl.qq.com/es/stats.shtml?bmid=8401     LPL  2022  Spring   \n",
       "\n",
       "    playoffs                 date  game  patch  ...  opp_csat15 golddiffat15  \\\n",
       "10         0  2022-01-10 07:44:08     1  12.01  ...       510.0        107.0   \n",
       "11         0  2022-01-10 07:44:08     1  12.01  ...       487.0       -107.0   \n",
       "22         0  2022-01-10 08:38:24     1  12.01  ...       555.0      -1763.0   \n",
       "23         0  2022-01-10 08:38:24     1  12.01  ...       533.0       1763.0   \n",
       "34         0  2022-01-10 09:24:26     1  12.01  ...         NaN          NaN   \n",
       "\n",
       "   xpdiffat15 csdiffat15 killsat15 assistsat15 deathsat15 opp_killsat15  \\\n",
       "10    -1617.0      -23.0       5.0        10.0        6.0           6.0   \n",
       "11     1617.0       23.0       6.0        18.0        5.0           5.0   \n",
       "22     -906.0      -22.0       1.0         1.0        3.0           3.0   \n",
       "23      906.0       22.0       3.0         3.0        1.0           1.0   \n",
       "34        NaN        NaN       NaN         NaN        NaN           NaN   \n",
       "\n",
       "   opp_assistsat15 opp_deathsat15  \n",
       "10            18.0            5.0  \n",
       "11            10.0            6.0  \n",
       "22             3.0            1.0  \n",
       "23             1.0            3.0  \n",
       "34             NaN            NaN  \n",
       "\n",
       "[5 rows x 123 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove all the player rows, keeping only the team statistics rows\n",
    "full_matches = pd.read_csv('2022_LoL_esports_match_data_from_OraclesElixir_20221108.csv')\n",
    "team_stat = full_matches[full_matches['position'] == 'team']\n",
    "team_stat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ESPORTSTMNT03_2788015', 'ESPORTSTMNT04_2170436', 'ESPORTSTMNT05_2980802'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abnormal_matches = set()\n",
    "\n",
    "# Matches that take less than 15 minutes\n",
    "abnormal_matches.update(team_stat[team_stat['gamelength'] <= 15*60]['gameid'].values)\n",
    "# Matches that have no winner\n",
    "no_winner = team_stat.groupby('gameid')['result'].apply(sum)\n",
    "abnormal_matches.update(no_winner[no_winner == 0].index)\n",
    "\n",
    "abnormal_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the abnormal matches\n",
    "for id in abnormal_matches:\n",
    "    team_stat = team_stat[team_stat['gameid'] != id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>result</th>\n",
       "      <th>patch</th>\n",
       "      <th>league</th>\n",
       "      <th>side</th>\n",
       "      <th>goldat15</th>\n",
       "      <th>xpat15</th>\n",
       "      <th>csat15</th>\n",
       "      <th>opp_goldat15</th>\n",
       "      <th>opp_xpat15</th>\n",
       "      <th>opp_csat15</th>\n",
       "      <th>golddiffat15</th>\n",
       "      <th>xpdiffat15</th>\n",
       "      <th>csdiffat15</th>\n",
       "      <th>killsat15</th>\n",
       "      <th>assistsat15</th>\n",
       "      <th>deathsat15</th>\n",
       "      <th>opp_killsat15</th>\n",
       "      <th>opp_assistsat15</th>\n",
       "      <th>opp_deathsat15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>12.01</td>\n",
       "      <td>LCK CL</td>\n",
       "      <td>Blue</td>\n",
       "      <td>24806.0</td>\n",
       "      <td>28001.0</td>\n",
       "      <td>487.0</td>\n",
       "      <td>24699.0</td>\n",
       "      <td>29618.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>-1617.0</td>\n",
       "      <td>-23.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>12.01</td>\n",
       "      <td>LCK CL</td>\n",
       "      <td>Red</td>\n",
       "      <td>24699.0</td>\n",
       "      <td>29618.0</td>\n",
       "      <td>510.0</td>\n",
       "      <td>24806.0</td>\n",
       "      <td>28001.0</td>\n",
       "      <td>487.0</td>\n",
       "      <td>-107.0</td>\n",
       "      <td>1617.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>12.01</td>\n",
       "      <td>LCK CL</td>\n",
       "      <td>Blue</td>\n",
       "      <td>23522.0</td>\n",
       "      <td>28848.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>25285.0</td>\n",
       "      <td>29754.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>-1763.0</td>\n",
       "      <td>-906.0</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>12.01</td>\n",
       "      <td>LCK CL</td>\n",
       "      <td>Red</td>\n",
       "      <td>25285.0</td>\n",
       "      <td>29754.0</td>\n",
       "      <td>555.0</td>\n",
       "      <td>23522.0</td>\n",
       "      <td>28848.0</td>\n",
       "      <td>533.0</td>\n",
       "      <td>1763.0</td>\n",
       "      <td>906.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>12.01</td>\n",
       "      <td>LCK CL</td>\n",
       "      <td>Blue</td>\n",
       "      <td>24795.0</td>\n",
       "      <td>31342.0</td>\n",
       "      <td>560.0</td>\n",
       "      <td>23604.0</td>\n",
       "      <td>29044.0</td>\n",
       "      <td>545.0</td>\n",
       "      <td>1191.0</td>\n",
       "      <td>2298.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    result  patch  league  side  goldat15   xpat15  csat15  opp_goldat15  \\\n",
       "10       0  12.01  LCK CL  Blue   24806.0  28001.0   487.0       24699.0   \n",
       "11       1  12.01  LCK CL   Red   24699.0  29618.0   510.0       24806.0   \n",
       "22       0  12.01  LCK CL  Blue   23522.0  28848.0   533.0       25285.0   \n",
       "23       1  12.01  LCK CL   Red   25285.0  29754.0   555.0       23522.0   \n",
       "46       1  12.01  LCK CL  Blue   24795.0  31342.0   560.0       23604.0   \n",
       "\n",
       "    opp_xpat15  opp_csat15  golddiffat15  xpdiffat15  csdiffat15  killsat15  \\\n",
       "10     29618.0       510.0         107.0     -1617.0       -23.0        5.0   \n",
       "11     28001.0       487.0        -107.0      1617.0        23.0        6.0   \n",
       "22     29754.0       555.0       -1763.0      -906.0       -22.0        1.0   \n",
       "23     28848.0       533.0        1763.0       906.0        22.0        3.0   \n",
       "46     29044.0       545.0        1191.0      2298.0        15.0        3.0   \n",
       "\n",
       "    assistsat15  deathsat15  opp_killsat15  opp_assistsat15  opp_deathsat15  \n",
       "10         10.0         6.0            6.0             18.0             5.0  \n",
       "11         18.0         5.0            5.0             10.0             6.0  \n",
       "22          1.0         3.0            3.0              3.0             1.0  \n",
       "23          3.0         1.0            1.0              1.0             3.0  \n",
       "46          8.0         1.0            1.0              1.0             3.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only the basic information columns, and the statistics at 15th minute\n",
    "stats_15 = team_stat[[\n",
    "    'result','patch','league','side', # basic information of the match\n",
    "    'goldat15','xpat15','csat15','opp_goldat15','opp_xpat15','opp_csat15',\n",
    "    'golddiffat15','xpdiffat15','csdiffat15',\n",
    "    'killsat15','assistsat15','deathsat15',\n",
    "    'opp_killsat15','opp_assistsat15','opp_deathsat15']]\n",
    "\n",
    "# Remove the rows where the statistics at 15th minute is missing\n",
    "stats_15 = stats_15[pd.notna(stats_15['goldat15'])]\n",
    "stats_15.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy score is: 0.603019877675841\n"
     ]
    }
   ],
   "source": [
    "# Columns with numeric features\n",
    "stats_15_col = ['killsat15', 'assistsat15', 'deathsat15', 'opp_killsat15', 'opp_assistsat15', 'opp_deathsat15']\n",
    "\n",
    "# Splitting the test and train sets\n",
    "X = stats_15.drop(['result'], axis=1)\n",
    "y = stats_15['result']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# The pre-processing Transformer\n",
    "preproc = ColumnTransformer([\n",
    "    # One hot encode the nomial feature\n",
    "    ('one-hot', OneHotEncoder(), ['league']),\n",
    "    # Leave the numeric features as is\n",
    "    ('numeric_cols', FunctionTransformer(lambda x: x,validate=False), stats_15_col)\n",
    "    ], \n",
    "    # Drop the rest columns\n",
    "    remainder='drop') \n",
    "\n",
    "# The Pipeline that do the prediction\n",
    "pl = Pipeline([\n",
    "    ('pre-processing', preproc),\n",
    "    ('dtc', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Making predictions\n",
    "pl.fit(X_train,y_train)\n",
    "pred = pl.predict(X_test)\n",
    "print('The accuracy score is:', accuracy_score(y_true=y_test, y_pred=pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hyperparameters I want to test with\n",
    "hyperparameters = {\n",
    "    'dtc__max_depth': [2, 3, 4, 5, 7, 10, 13, 15, 18, None], \n",
    "    'dtc__min_samples_split': [2, 3, 5, 7, 10, 15, 20],\n",
    "    'dtc__criterion': ['gini', 'entropy']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6620795107033639"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The numeric features used (statistics at 15th minute)\n",
    "stats_15_col = [\n",
    "    'golddiffat15','xpdiffat15','csdiffat15',\n",
    "    'killsat15','assistsat15','deathsat15',\n",
    "    'opp_killsat15','opp_assistsat15','opp_deathsat15']\n",
    "\n",
    "# Split the train and test sets again \n",
    "X = stats_15.drop(['result'], axis=1)\n",
    "y = stats_15['result']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "# The pre-processing Transformer\n",
    "preproc = ColumnTransformer([\n",
    "    # Standarize the numeric features\n",
    "    ('numeric_cols', StandardScaler(), stats_15_col),\n",
    "    # One-hot encode the nomial features\n",
    "    ('ont-hot', OneHotEncoder(), ['league','patch','side'])\n",
    "    ],\n",
    "    # Drop the rest of the columns\n",
    "    remainder='drop')\n",
    "\n",
    "# The Pipeline that do the prediction\n",
    "pl = Pipeline([\n",
    "    ('pre-processing', preproc),\n",
    "    ('dtc', DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# Making prediction without using any hyperparameters\n",
    "pl.fit(X_train,y_train)\n",
    "pred = pl.predict(X_test)\n",
    "accuracy_score(y_true=y_test, y_pred=pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dtc__criterion': 'gini', 'dtc__max_depth': 4, 'dtc__min_samples_split': 2}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search for the best hyperparameters using GridSearchCV\n",
    "grids = GridSearchCV(pl, param_grid=hyperparameters, return_train_score=True)\n",
    "grids.fit(X_train, y_train)\n",
    "grids.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.752420998980632"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The accuracy score on the train set using the best hyperparameter combination\n",
    "grids.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7356651376146789"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The accuracy score on the test set using the best hyperparameter combination\n",
    "grids.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7205567451820128, 0.7495248574572372, -0.028968112275224334)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splits the dataset into two subsets\n",
    "\n",
    "# The LCK subset\n",
    "lck = stats_15[stats_15['league'] == 'LCK']\n",
    "X_lck = lck.drop(['result'], axis=1)\n",
    "y_lck = lck['result']\n",
    "\n",
    "# The other leagues subset\n",
    "other = stats_15[stats_15['league'] != 'LCK']\n",
    "X_other = other.drop(['result'], axis=1)\n",
    "y_other = other['result']\n",
    "\n",
    "# Observed statistics\n",
    "observed_stat = grids.score(X_lck, y_lck) - grids.score(X_other, y_other)\n",
    "grids.score(X_lck, y_lck), grids.score(X_other, y_other), observed_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of repetitions\n",
    "n_repetitions = 500\n",
    "\n",
    "accuracy_diffs = []\n",
    "for _ in range(n_repetitions):\n",
    "    \n",
    "    # Step 1: Shuffle the league column\n",
    "    shuffled_league = (\n",
    "        stats_15['league']\n",
    "        .sample(frac=1)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    \n",
    "    # Step 2: Put the shuffled column in a DataFrame\n",
    "    shuffled = (\n",
    "        stats_15\n",
    "        .assign(**{'shuffled_league': shuffled_league})\n",
    "    )\n",
    "    \n",
    "    # Step 3: Compute the accuracy difference of the predictions on each subset\n",
    "    lck = shuffled[shuffled['shuffled_league'] == 'LCK']\n",
    "    X_lck = lck.drop(['result'], axis=1)\n",
    "    y_lck = lck['result']\n",
    "\n",
    "    other = shuffled[shuffled['shuffled_league'] != 'LCK']\n",
    "    X_other = other.drop(['result'], axis=1)\n",
    "    y_other = other['result']\n",
    "\n",
    "    accuracy_diff = grids.score(X_lck, y_lck) - grids.score(X_other, y_other)\n",
    "    \n",
    "    # Step 4: Store the result\n",
    "    accuracy_diffs.append(accuracy_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.258"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the P-Value\n",
    "pval = np.mean(accuracy_diffs <= observed_stat)\n",
    "pval"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "1940850b94332e03c80e23b2548941574ef8dfbc6d83d5d3f2de339d61f011e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
